{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMIVFudpcdNz6EQIO1PQaFb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joosk3R/jskRprac/blob/main/efficientnet_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "from torchsummary import summary\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "LRRwzGdngSiY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "se_channels = max(1, 100)\n"
      ],
      "metadata": {
        "id": "P6w4rGDayZTG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "se_channels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cow0mh3mybgn",
        "outputId": "7b602593-d4fc-42cf-c919-258e0ef5192d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 논문에서 정한 reduction_ratio=0.25\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, r=0.25):\n",
        "        super().__init__()\n",
        "\n",
        "        # se_channels : reduce layer out channels 계산\n",
        "        se_channels = max(1, int(in_channels*r))\n",
        "\n",
        "        self.se = nn.Sequential(# squeeze\n",
        "                                nn.AdaptiveAvgPool2d(1),\n",
        "                                # excitation\n",
        "                                nn.Conv2d(in_channels, se_channels, kernel_size=1),\n",
        "                                nn.SiLU(),\n",
        "                                nn.Conv2d(se_channels, in_channels, kernel_size=1),\n",
        "                                nn.Sigmoid()\n",
        "                               )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.se(x)"
      ],
      "metadata": {
        "id": "MGeBnJYnyWTc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MBConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, expand, kernel_size, stride=1, r=0.25, dropout_rate=0.2, bias=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # 변수 설정\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.expand = expand\n",
        "\n",
        "        # skip connection 사용을 위한 조건 지정\n",
        "        self.use_residual = in_channels == out_channels and stride == 1\n",
        "\n",
        "        # 논문에서 수행한 BatchNorm, SiLU 적용\n",
        "        # stage1. Expansion\n",
        "        expand_channels = in_channels*expand\n",
        "        self.expansion = nn.Sequential(nn.Conv2d(in_channels, expand_channels, 1, bias=False),\n",
        "                                       nn.BatchNorm2d(expand_channels, momentum=0.99),\n",
        "                                       nn.SiLU(),\n",
        "                                      )\n",
        "\n",
        "        # stage2. Depth-wise convolution\n",
        "        self.depth_wise = nn.Sequential(nn.Conv2d(expand_channels, expand_channels, kernel_size=kernel_size, stride=1, padding=1, groups=expand_channels),\n",
        "                                        nn.BatchNorm2d(expand_channels, momentum=0.99),\n",
        "                                        nn.SiLU(),\n",
        "                                       )\n",
        "\n",
        "        # stage3. Squeeze and Excitation\n",
        "        self.se_block = SEBlock(expand_channels, r)\n",
        "\n",
        "        # stage4. Point-wise convolution\n",
        "        self.point_wise = nn.Sequential(nn.Conv2d(expand_channels, out_channels, 1, 1, bias=False),\n",
        "                                        nn.BatchNorm2d(out_channels, momentum=0.99)\n",
        "                                       )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # stage1\n",
        "        if self.expand != 1:\n",
        "            x = self.expansion(x)\n",
        "\n",
        "        # stage2\n",
        "        x = self.depth_wise(x)\n",
        "\n",
        "        # stage3\n",
        "        x = self.se_block(x)\n",
        "\n",
        "        # stage4\n",
        "        x = self.point_wise(x)\n",
        "\n",
        "        # stage5 skip connection\n",
        "        res = x\n",
        "\n",
        "        if self.use_residual:\n",
        "            if self.training and (self.dropout_rate is not None):\n",
        "                x = F.dropout2d(input=x, p=self.dropout_rate, training=self.training, inplace=True)\n",
        "\n",
        "            x = x + res\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "SKnwbvxZ7SKT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, num_classes, width, depth, resolution, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # stage1\n",
        "        out_ch = int(32*width)\n",
        "        self.stage1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=out_ch, kernel_size=3, stride=2, padding=1),\n",
        "                                    nn.BatchNorm2d(out_ch, momentum=0.99))\n",
        "\n",
        "        # stage2\n",
        "        self.stage2 = nn.Sequential(MBConvBlock(in_channels=out_ch, out_channels=16, expand=1, kernel_size=3, stride=1, dropout_rate=dropout))\n",
        "\n",
        "        # stage3\n",
        "        self.stage3 = nn.Sequential(MBConvBlock(in_channels=16, out_channels=24, expand=6, kernel_size=3, stride=2, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=24, out_channels=24, expand=6, kernel_size=3, stride=1, dropout_rate=dropout),\n",
        "                                   )\n",
        "\n",
        "        # stage4\n",
        "        self.stage4 = nn.Sequential(MBConvBlock(in_channels=24, out_channels=40, expand=6, kernel_size=5, stride=2, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=40, out_channels=40, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                   )\n",
        "\n",
        "        # stage5\n",
        "        self.stage5 = nn.Sequential(MBConvBlock(in_channels=40, out_channels=80, expand=6, kernel_size=3, stride=2, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=80, out_channels=80, expand=6, kernel_size=3, stride=1, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=80, out_channels=80, expand=6, kernel_size=3, stride=1, dropout_rate=dropout),\n",
        "                                   )\n",
        "\n",
        "        # stage6\n",
        "        self.stage6 = nn.Sequential(MBConvBlock(in_channels=80, out_channels=112, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=112, out_channels=112, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=112, out_channels=112, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                   )\n",
        "\n",
        "        # stage7\n",
        "        self.stage7 = nn.Sequential(MBConvBlock(in_channels=112, out_channels=192, expand=6, kernel_size=5, stride=2, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=192, out_channels=192, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=192, out_channels=192, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=192, out_channels=192, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                   )\n",
        "\n",
        "        # stage8\n",
        "        self.stage8 = nn.Sequential(MBConvBlock(in_channels=192, out_channels=320, expand=6, kernel_size=3, stride=1, dropout_rate=dropout))\n",
        "\n",
        "        # stage9\n",
        "        self.last_channels = math.ceil(1280*width)\n",
        "        self.stage9 = nn.Conv2d(in_channels=320, out_channels=self.last_channels, kernel_size=1)\n",
        "\n",
        "        # result\n",
        "        self.out_layer = nn.Linear(self.last_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = self.stage5(x)\n",
        "        x = self.stage6(x)\n",
        "        x = self.stage7(x)\n",
        "        x = self.stage8(x)\n",
        "        x = self.stage9(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).view(-1, self.last_channels)\n",
        "        x = self.out_layer(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "5L7SVCzy9GSC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNet2(nn.Module):\n",
        "    def __init__(self, num_classes, width, depth, resolution, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # stage1\n",
        "        out_ch = int(32*width)\n",
        "        self.stage1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=out_ch, kernel_size=3, stride=2, padding=1),\n",
        "                                    nn.BatchNorm2d(out_ch, momentum=0.99))\n",
        "\n",
        "        # stage2\n",
        "        self.stage2 = nn.Sequential(MBConvBlock(in_channels=out_ch, out_channels=16, expand=1, kernel_size=3, stride=1, dropout_rate=dropout))\n",
        "\n",
        "        # stage3\n",
        "        self.stage3 = nn.Sequential(MBConvBlock(in_channels=16, out_channels=24, expand=6, kernel_size=3, stride=2, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=24, out_channels=24, expand=6, kernel_size=3, stride=1, dropout_rate=dropout),\n",
        "                                   )\n",
        "\n",
        "        # stage4\n",
        "        self.stage4 = nn.Sequential(MBConvBlock(in_channels=24, out_channels=40, expand=6, kernel_size=5, stride=2, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=40, out_channels=40, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                   )\n",
        "\n",
        "        # stage5\n",
        "        self.stage5 = nn.Sequential(MBConvBlock(in_channels=40, out_channels=80, expand=6, kernel_size=3, stride=2, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=80, out_channels=80, expand=6, kernel_size=3, stride=1, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=80, out_channels=80, expand=6, kernel_size=3, stride=1, dropout_rate=dropout),\n",
        "                                   )\n",
        "\n",
        "        # stage6\n",
        "        self.stage6 = nn.Sequential(MBConvBlock(in_channels=80, out_channels=112, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=112, out_channels=112, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=112, out_channels=112, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                   )\n",
        "\n",
        "        # stage7\n",
        "        self.stage7 = nn.Sequential(MBConvBlock(in_channels=112, out_channels=192, expand=6, kernel_size=5, stride=2, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=192, out_channels=192, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=192, out_channels=192, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                    MBConvBlock(in_channels=192, out_channels=192, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
        "                                   )\n",
        "\n",
        "        # stage8\n",
        "        self.stage8 = nn.Sequential(MBConvBlock(in_channels=192, out_channels=320, expand=6, kernel_size=3, stride=1, dropout_rate=dropout))\n",
        "\n",
        "        # stage9\n",
        "        self.last_channels = math.ceil(1280*width)\n",
        "        self.stage9 = nn.Conv2d(in_channels=320, out_channels=self.last_channels, kernel_size=1)\n",
        "\n",
        "        # result\n",
        "        self.out_layer = nn.Linear(self.last_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = self.stage5(x)\n",
        "        x = self.stage6(x)\n",
        "        x = self.stage7(x)\n",
        "        x = self.stage8(x)\n",
        "        x = self.stage9(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).view(-1, self.last_channels)\n",
        "        x = self.out_layer(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "zqzYwzH8-Udv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# efficientnet 모델 b0 ~ b7\n",
        "def efficientnet_b0(num_classes=10):\n",
        "    return EfficientNet(num_classes=num_classes, width=1.0, depth=1.0, resolution=224, dropout=0.2)\n",
        "\n",
        "def efficientnet_b1(num_classes=10):\n",
        "    return EfficientNet(num_classes=num_classes, width=1.0, depth=1.1, resolution=240, dropout=0.2)\n",
        "\n",
        "def efficientnet_b2(num_classes=10):\n",
        "    return EfficientNet(num_classes=num_classes, width=1.1, depth=1.2, resolution=260, dropout=0.3)\n",
        "\n",
        "def efficientnet_b3(num_classes=10):\n",
        "    return EfficientNet(num_classes=num_classes, width=1.2, depth=1.4, resolution=300, dropout=0.3)\n",
        "\n",
        "def efficientnet_b4(num_classes=10):\n",
        "    return EfficientNet(num_classes=num_classes, width=1.4, depth=1.8, resolution=380, dropout=0.4)\n",
        "\n",
        "def efficientnet_b5(num_classes=10):\n",
        "    return EfficientNet(num_classes=num_classes, width=1.6, depth=2.2, resolution=456, dropout=0.4)\n",
        "\n",
        "def efficientnet_b6(num_classes=10):\n",
        "    return EfficientNet(num_classes=num_classes, width=1.8, depth=2.6, resolution=528, dropout=0.5)\n",
        "\n",
        "def efficientnet_b7(num_classes=10):\n",
        "    return EfficientNet(num_classes=num_classes, width=2.0, depth=3.1, resolution=600, dropout=0.5)"
      ],
      "metadata": {
        "id": "-aTvYlkXDmvu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    x = torch.randn(4, 3, 224, 224).to(device)\n",
        "    model = efficientnet_b0().to(device)\n",
        "    output = model(x)\n",
        "    print('output size:', output.size())\n",
        "\n",
        "    summary(model, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "id": "WnMDIuTUCWhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 변수설정\n",
        "batch_size = 128\n",
        "n_epochs = 10\n",
        "lr = 0.1\n",
        "data_dir = '/data/cifa10'"
      ],
      "metadata": {
        "id": "2aDRXi0xCc4G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFVJ2rK0GCoK",
        "outputId": "7d88b558-9ff9-4078-851c-d108a9cf3dfc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=9108c63e829c95526a7f11fa0a7ff627a4c3a0a1db8c4fd2b1d3617a957bf531\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: efficientnet_pytorch\n",
            "Successfully installed efficientnet_pytorch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델호출\n",
        "# 위에서 내가 만든 모델 사용\n",
        "\n",
        "\n",
        "# 모델 패키지 사용\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "model = EfficientNet.from_pretrained('efficientnet-b0').to(device)\n",
        "\n",
        "# 비용함수, 옵티마이저 정의\n",
        "cost = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLop9DkwCsqT",
        "outputId": "e1210be4-ef38-46d7-c8fe-9c806bbe8e36"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 transform\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                               ])\n",
        "\n",
        "# 데이터 불러오기\n",
        "trainset = torchvision.datasets.CIFAR10(root=data_dir+'/train', train=True, download=True, transform=transform)\n",
        "valset = torchvision.datasets.CIFAR10(root=data_dir+'/test', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDUprxqoFa20",
        "outputId": "682b5f89-b3fa-41f6-8b86-7e55f29746e8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /data/cifa10/train/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12850397.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /data/cifa10/train/cifar-10-python.tar.gz to /data/cifa10/train\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /data/cifa10/test/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 13095237.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /data/cifa10/test/cifar-10-python.tar.gz to /data/cifa10/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "for epoch in range(n_epochs):\n",
        "    avg_cost = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    for X, Y in trainloader:\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        # 가설\n",
        "        hypothesis = model(X)\n",
        "\n",
        "        # loss\n",
        "        loss = cost(hypothesis, Y)\n",
        "\n",
        "        # update\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # avg_cost\n",
        "        avg_cost += loss / len(trainloader)\n",
        "        correct = torch.argmax(hypothesis, 1) == Y\n",
        "        accuracy = correct.float().mean()\n",
        "\n",
        "    print(f'Epoch {epoch}/{n_epochs}\\tLoss {avg_cost:.4f}\\tAcc {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "K9CQWMUXGSk8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}